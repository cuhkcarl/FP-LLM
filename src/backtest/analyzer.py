"""
æ•°æ®é©±åŠ¨çš„å›æµ‹åˆ†æç³»ç»Ÿ V2

ç‰¹ç‚¹ï¼š
- å®Œå…¨åŸºäºæ•°æ®ç”ŸæˆæŠ¥å‘Š
- å¯é…ç½®çš„è¯„çº§æ ‡å‡†å’Œæ¨¡æ¿
- æ”¯æŒå¤šç­–ç•¥è‡ªåŠ¨å¯¹æ¯”
- åŠ¨æ€åŠ è½½çƒå‘˜ä¿¡æ¯
"""

import json
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import yaml


class ConfigurableAnalyzer:
    """
    å¯é…ç½®çš„å›æµ‹åˆ†æå™¨

    é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰ï¼š
    - è¯„çº§æ ‡å‡†
    - æŠ¥å‘Šæ¨¡æ¿
    - ç­–ç•¥ç‰¹å¾æè¿°
    """

    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.results = {}
        self.player_data = {}

        # åŠ è½½é…ç½®
        if config_path and Path(config_path).exists():
            with open(config_path, encoding="utf-8") as f:
                self.config = yaml.safe_load(f)
        else:
            self.config = self._get_default_config()

    def _get_default_config(self) -> dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "grading_standards": {
                "performance": [
                    {"min": 60, "grade": "A", "label": "ä¼˜ç§€", "description": "è¡¨ç°è¶…å‡ºé¢„æœŸ"},
                    {"min": 55, "grade": "B", "label": "è‰¯å¥½", "description": "è¡¨ç°ç¬¦åˆé¢„æœŸ"},
                    {"min": 50, "grade": "C", "label": "ä¸€èˆ¬", "description": "è¡¨ç°ç•¥ä½äºé¢„æœŸ"},
                    {"min": 0, "grade": "D", "label": "è¾ƒå·®", "description": "è¡¨ç°æ˜æ˜¾ä½äºé¢„æœŸ"},
                ],
                "consistency": [
                    {"min": 0.8, "label": "é«˜ç¨³å®šæ€§"},
                    {"min": 0.6, "label": "ä¸­ç­‰ç¨³å®šæ€§"},
                    {"min": 0.0, "label": "ä½ç¨³å®šæ€§"},
                ],
            },
            "benchmarks": {
                "fpl_average_per_gw": 52.5,
                "good_consistency_threshold": 0.7,
                "excellent_avg_threshold": 60,
            },
            "strategy_profiles": {
                "S0": {
                    "name": "é™æ€åŸºå‡†ç­–ç•¥",
                    "characteristics": [
                        "åŸºäºç¾¤ä½“æ™ºæ…§çš„é¢„è®¡ç®—æœ€ä¼˜é˜µå®¹",
                        "å›ºå®š15äººé˜µå®¹ï¼Œæ— è½¬ä¼šæ“ä½œ",
                        "æ¯å‘¨é€‰æ‹©æœ€ä½³11äººé¦–å‘å’Œé˜Ÿé•¿",
                        "ç­–ç•¥æ‰§è¡Œç®€å•ï¼Œé£é™©å¯æ§",
                    ],
                    "expected_performance": "ä¸­ç­‰åä¸Š",
                    "risk_level": "ä½",
                }
            },
        }

    def load_result(self, strategy_name: str, result_path: str) -> None:
        """åŠ è½½ç­–ç•¥å›æµ‹ç»“æœ"""
        with open(result_path, encoding="utf-8") as f:
            raw_data = yaml.safe_load(f)

        # å¤„ç†æ•°æ®
        processed = self._process_raw_data(raw_data, strategy_name)
        self.results[strategy_name] = processed

        print(f"âœ… å·²åŠ è½½ {strategy_name} ç­–ç•¥ç»“æœ")
        print(f"   - èµ›å­£: {processed['summary']['season']}")
        print(f"   - å®Œæˆå‘¨æ•°: {processed['summary']['gameweeks_completed']}")
        print(f"   - æ€»å¾—åˆ†: {processed['summary']['total_points']}")

    def load_player_mapping(self, season: str = "2023-24") -> None:
        """åŠ è½½çƒå‘˜IDåˆ°å§“åçš„æ˜ å°„"""
        try:
            # ä»S0é…ç½®ä¸­è·å–çƒå‘˜ä¿¡æ¯
            s0_config_path = "/Users/carl/workspace/FP-LLM/configs/backtest/s0_optimal_squad.yaml"
            if Path(s0_config_path).exists():
                with open(s0_config_path, encoding="utf-8") as f:
                    s0_data = yaml.safe_load(f)

                players_info = s0_data.get("optimal_squad_2023_24", {}).get("players", [])
                for player in players_info:
                    self.player_data[player["id"]] = {
                        "name": player["name"],
                        "position": player["position"],
                        "team": player["team"],
                    }

                print(f"âœ… å·²åŠ è½½ {len(self.player_data)} åçƒå‘˜ä¿¡æ¯")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½çƒå‘˜ä¿¡æ¯å¤±è´¥: {e}")

    def _process_raw_data(self, raw_data: dict, strategy_name: str) -> dict:
        """å¤„ç†åŸå§‹æ•°æ®"""
        # æå–gameweekæ•°æ®
        gw_data = []
        for gw_result in raw_data.get("gameweek_results", []):
            gw_data.append(
                {
                    "gw": gw_result["gw"],
                    "score": gw_result["score"],
                    "cumulative_score": gw_result["cumulative_score"],
                    "captain_id": gw_result["lineup"]["captain_id"],
                    "formation": gw_result["lineup"]["formation"],
                }
            )

        df = pd.DataFrame(gw_data)

        return {
            "strategy_name": strategy_name,
            "raw_data": raw_data,
            "gameweek_df": df,
            "summary": {
                "season": raw_data.get("season"),
                "total_points": raw_data.get("total_points"),
                "gameweeks_completed": raw_data.get("gameweeks_completed"),
                "average_points_per_gw": raw_data.get("average_points_per_gw"),
                "points_std": raw_data.get("points_std"),
            },
        }

    def analyze_strategy(self, strategy_name: str) -> dict:
        """åˆ†æå•ä¸ªç­–ç•¥"""
        if strategy_name not in self.results:
            raise KeyError(f"ç­–ç•¥ {strategy_name} å°šæœªåŠ è½½")

        result = self.results[strategy_name]
        df = result["gameweek_df"]

        analysis = {
            "basic_metrics": self._compute_basic_metrics(df),
            "performance_rating": self._rate_performance(result["summary"]),
            "consistency_analysis": self._analyze_consistency(df),
            "captain_analysis": self._analyze_captains(df),
            "formation_analysis": self._analyze_formations(df),
            "trend_analysis": self._analyze_trends(df),
            "comparative_analysis": self._compare_to_benchmarks(result["summary"]),
        }

        return analysis

    def _compute_basic_metrics(self, df: pd.DataFrame) -> dict:
        """è®¡ç®—åŸºç¡€æŒ‡æ ‡"""
        scores = df["score"]
        return {
            "total_gameweeks": len(df),
            "total_points": scores.sum(),
            "average_points": scores.mean(),
            "median_points": scores.median(),
            "std_points": scores.std(),
            "min_points": scores.min(),
            "max_points": scores.max(),
            "percentiles": {
                "25th": scores.quantile(0.25),
                "75th": scores.quantile(0.75),
                "90th": scores.quantile(0.90),
            },
            "score_distribution": {
                "excellent_weeks": (scores >= 70).sum(),  # 70+åˆ†çš„å‘¨æ•°
                "good_weeks": ((scores >= 55) & (scores < 70)).sum(),  # 55-69åˆ†
                "average_weeks": ((scores >= 40) & (scores < 55)).sum(),  # 40-54åˆ†
                "poor_weeks": (scores < 40).sum(),  # <40åˆ†
            },
        }

    def _rate_performance(self, summary: dict) -> dict:
        """è¯„çº§è¡¨ç°"""
        avg_score = summary["average_points_per_gw"]

        # æ ¹æ®é…ç½®çš„æ ‡å‡†è¯„çº§
        grade_info = None
        for standard in self.config["grading_standards"]["performance"]:
            if avg_score >= standard["min"]:
                grade_info = standard
                break

        if not grade_info:
            grade_info = self.config["grading_standards"]["performance"][-1]

        return {
            "grade": grade_info["grade"],
            "label": grade_info["label"],
            "description": grade_info["description"],
            "score": avg_score,
            "vs_fpl_average": avg_score - self.config["benchmarks"]["fpl_average_per_gw"],
        }

    def _analyze_consistency(self, df: pd.DataFrame) -> dict:
        """åˆ†æä¸€è‡´æ€§"""
        scores = df["score"]
        avg_score = scores.mean()
        cv = scores.std() / avg_score if avg_score > 0 else 0
        consistency_score = 1 / (1 + cv)

        # è¯„çº§ä¸€è‡´æ€§
        consistency_label = None
        for standard in self.config["grading_standards"]["consistency"]:
            if consistency_score >= standard["min"]:
                consistency_label = standard["label"]
                break

        return {
            "consistency_score": consistency_score,
            "coefficient_of_variation": cv,
            "label": consistency_label,
            "above_average_weeks": (scores >= avg_score).sum(),
            "below_average_weeks": (scores < avg_score).sum(),
            "volatility_assessment": "low" if cv < 0.3 else "medium" if cv < 0.5 else "high",
        }

    def _analyze_captains(self, df: pd.DataFrame) -> dict:
        """åˆ†æé˜Ÿé•¿é€‰æ‹©"""
        captain_counts = df["captain_id"].value_counts()
        total_games = len(df)

        # è½¬æ¢ä¸ºçƒå‘˜å§“å
        captain_info = []
        for captain_id, count in captain_counts.items():
            player_info = self.player_data.get(captain_id, {})
            captain_info.append(
                {
                    "id": captain_id,
                    "name": player_info.get("name", f"Player_{captain_id}"),
                    "position": player_info.get("position", "Unknown"),
                    "team": player_info.get("team", "Unknown"),
                    "games_captained": count,
                    "percentage": (count / total_games) * 100,
                }
            )

        # æŒ‰ä½¿ç”¨é¢‘ç‡æ’åº
        captain_info.sort(key=lambda x: x["games_captained"], reverse=True)

        return {
            "total_different_captains": len(captain_counts),
            "most_used_captain": captain_info[0] if captain_info else None,
            "captain_distribution": captain_info,
            "captain_diversity": len(captain_counts) / total_games,  # å¤šæ ·æ€§æŒ‡æ ‡
        }

    def _analyze_formations(self, df: pd.DataFrame) -> dict:
        """åˆ†æé˜µå‹ä½¿ç”¨"""
        formation_counts = df["formation"].value_counts()
        total_games = len(df)

        formation_info = []
        for formation, count in formation_counts.items():
            formation_info.append(
                {
                    "formation": formation,
                    "games_used": count,
                    "percentage": (count / total_games) * 100,
                }
            )

        return {
            "total_formations_used": len(formation_counts),
            "primary_formation": formation_info[0]["formation"] if formation_info else None,
            "formation_distribution": formation_info,
            "formation_diversity": len(formation_counts) / total_games,
        }

    def _analyze_trends(self, df: pd.DataFrame) -> dict:
        """åˆ†æè¶‹åŠ¿"""
        scores = df["score"].values
        gws = df["gw"].values

        if len(scores) > 1:
            # çº¿æ€§è¶‹åŠ¿
            trend_slope = np.polyfit(gws, scores, 1)[0]

            # ç§»åŠ¨å¹³å‡
            window_size = min(5, len(scores))
            rolling_avg = pd.Series(scores).rolling(window=window_size).mean()

            # åˆ†æ®µåˆ†æ
            mid_point = len(scores) // 2
            first_half_avg = scores[:mid_point].mean() if mid_point > 0 else scores.mean()
            second_half_avg = (
                scores[mid_point:].mean() if mid_point < len(scores) else scores.mean()
            )
        else:
            trend_slope = 0
            rolling_avg = pd.Series([scores[0]] if len(scores) > 0 else [0])
            first_half_avg = second_half_avg = scores[0] if len(scores) > 0 else 0

        # è¶‹åŠ¿åˆ¤æ–­
        if abs(trend_slope) < 0.1:
            trend_direction = "stable"
        elif trend_slope > 0:
            trend_direction = "improving"
        else:
            trend_direction = "declining"

        return {
            "trend_slope": trend_slope,
            "trend_direction": trend_direction,
            "rolling_average": rolling_avg.tolist(),
            "first_half_average": first_half_avg,
            "second_half_average": second_half_avg,
            "half_performance_change": second_half_avg - first_half_avg,
        }

    def _compare_to_benchmarks(self, summary: dict) -> dict:
        """ä¸åŸºå‡†å¯¹æ¯”"""
        avg_score = summary["average_points_per_gw"]
        benchmarks = self.config["benchmarks"]

        return {
            "vs_fpl_average": {
                "difference": avg_score - benchmarks["fpl_average_per_gw"],
                "percentage_better": ((avg_score / benchmarks["fpl_average_per_gw"]) - 1) * 100,
                "assessment": "above" if avg_score > benchmarks["fpl_average_per_gw"] else "below",
            },
            "vs_excellence_threshold": {
                "difference": avg_score - benchmarks["excellent_avg_threshold"],
                "reached_excellence": avg_score >= benchmarks["excellent_avg_threshold"],
            },
        }

    def generate_report(self, strategy_name: str, output_path: str = None) -> str:
        """ç”Ÿæˆæ•°æ®é©±åŠ¨çš„æŠ¥å‘Š"""
        if strategy_name not in self.results:
            raise KeyError(f"ç­–ç•¥ {strategy_name} å°šæœªåŠ è½½")

        result = self.results[strategy_name]
        analysis = self.analyze_strategy(strategy_name)

        # è·å–ç­–ç•¥é…ç½®ä¿¡æ¯
        strategy_profile = self.config["strategy_profiles"].get(
            strategy_name,
            {
                "name": f"{strategy_name}ç­–ç•¥",
                "characteristics": ["å¾…é…ç½®"],
                "expected_performance": "å¾…è¯„ä¼°",
                "risk_level": "å¾…è¯„ä¼°",
            },
        )

        report = self._generate_data_driven_report(
            strategy_name, result, analysis, strategy_profile
        )

        # ä¿å­˜æŠ¥å‘Š
        if output_path:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)

            with open(output_file, "w", encoding="utf-8") as f:
                f.write(report)

            print(f"ğŸ“Š æ•°æ®é©±åŠ¨æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

        return report

    def _generate_data_driven_report(
        self, strategy_name: str, result: dict, analysis: dict, profile: dict
    ) -> str:
        """ç”Ÿæˆå®Œå…¨åŸºäºæ•°æ®çš„æŠ¥å‘Š"""

        basic = analysis["basic_metrics"]
        performance = analysis["performance_rating"]
        consistency = analysis["consistency_analysis"]
        captain = analysis["captain_analysis"]
        formation = analysis["formation_analysis"]
        trends = analysis["trend_analysis"]
        benchmark = analysis["comparative_analysis"]

        # åŠ¨æ€ç”Ÿæˆç­–ç•¥ç‰¹å¾æè¿°
        captain_diversity_desc = (
            "é˜Ÿé•¿é€‰æ‹©å¤šæ ·åŒ–" if captain["captain_diversity"] > 0.3 else "é˜Ÿé•¿é€‰æ‹©ç›¸å¯¹å›ºå®š"
        )
        formation_diversity_desc = (
            "é˜µå‹ä½¿ç”¨çµæ´»" if formation["formation_diversity"] > 0.1 else "é˜µå‹ä½¿ç”¨å›ºå®š"
        )

        report = f"""# {profile['name']}å›æµ‹åˆ†ææŠ¥å‘Š

## ğŸ“‹ ç­–ç•¥æ¦‚è§ˆ

### åŸºæœ¬ä¿¡æ¯
- **ç­–ç•¥ä»£ç **: {strategy_name}
- **ç­–ç•¥åç§°**: {profile['name']}
- **åˆ†æèµ›å­£**: {result['summary']['season']}
- **å®Œæˆè¿›åº¦**: {basic['total_gameweeks']}/38å‘¨ ({basic['total_gameweeks']/38*100:.1f}%)
- **æ•°æ®å®Œæ•´æ€§**: {"âœ… å®Œæ•´" if basic['total_gameweeks'] == 38 else f"âš ï¸ ç¼ºå¤±{38-basic['total_gameweeks']}å‘¨æ•°æ®"}
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### ç­–ç•¥ç‰¹å¾
"""

        # åŠ¨æ€æ·»åŠ ç­–ç•¥ç‰¹å¾
        for characteristic in profile["characteristics"]:
            report += f"- {characteristic}\n"

        report += f"""
### æ ¸å¿ƒæŒ‡æ ‡æ¦‚è§ˆ
- **æ€»å¾—åˆ†**: {basic['total_points']:.0f}åˆ†
- **å¹³å‡å¾—åˆ†**: {basic['average_points']:.1f}åˆ†/å‘¨
- **è¡¨ç°è¯„çº§**: {performance['label']} ({performance['grade']}çº§)
- **ç¨³å®šæ€§**: {consistency['label']} (å¾—åˆ†: {consistency['consistency_score']:.3f})
- **ä¸»è¦ç‰¹å¾**: {captain_diversity_desc}, {formation_diversity_desc}

## ğŸ“Š è¡¨ç°åˆ†æ

### å¾—åˆ†ç»Ÿè®¡
- **æ€»å¾—åˆ†**: {basic['total_points']:.0f}åˆ†
- **å¹³å‡å¾—åˆ†**: {basic['average_points']:.1f}åˆ†/å‘¨
- **ä¸­ä½æ•°**: {basic['median_points']:.1f}åˆ†
- **æ ‡å‡†å·®**: {basic['std_points']:.1f}åˆ†
- **å¾—åˆ†åŒºé—´**: {basic['min_points']:.0f} - {basic['max_points']:.0f}åˆ†

### å¾—åˆ†åˆ†å¸ƒ
- **ä¼˜ç§€è¡¨ç°** (â‰¥70åˆ†): {basic['score_distribution']['excellent_weeks']}å‘¨ ({basic['score_distribution']['excellent_weeks']/basic['total_gameweeks']*100:.1f}%)
- **è‰¯å¥½è¡¨ç°** (55-69åˆ†): {basic['score_distribution']['good_weeks']}å‘¨ ({basic['score_distribution']['good_weeks']/basic['total_gameweeks']*100:.1f}%)
- **å¹³å‡è¡¨ç°** (40-54åˆ†): {basic['score_distribution']['average_weeks']}å‘¨ ({basic['score_distribution']['average_weeks']/basic['total_gameweeks']*100:.1f}%)
- **è¾ƒå·®è¡¨ç°** (<40åˆ†): {basic['score_distribution']['poor_weeks']}å‘¨ ({basic['score_distribution']['poor_weeks']/basic['total_gameweeks']*100:.1f}%)

### åˆ†ä½æ•°åˆ†æ
- **25%åˆ†ä½æ•°**: {basic['percentiles']['25th']:.1f}åˆ†
- **75%åˆ†ä½æ•°**: {basic['percentiles']['75th']:.1f}åˆ†
- **90%åˆ†ä½æ•°**: {basic['percentiles']['90th']:.1f}åˆ†

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

### èµ›å­£è¡¨ç°è¶‹åŠ¿
- **æ•´ä½“è¶‹åŠ¿**: {trends['trend_direction']} (æ–œç‡: {trends['trend_slope']:+.3f}åˆ†/å‘¨)
- **å‰åŠç¨‹å¹³å‡**: {trends['first_half_average']:.1f}åˆ†
- **ååŠç¨‹å¹³å‡**: {trends['second_half_average']:.1f}åˆ†
- **è¿›æ­¥å¹…åº¦**: {trends['half_performance_change']:+.1f}åˆ†

## ğŸ¯ ç¨³å®šæ€§è¯„ä¼°

### ä¸€è‡´æ€§æŒ‡æ ‡
- **ç¨³å®šæ€§è¯„çº§**: {consistency['label']}
- **ä¸€è‡´æ€§å¾—åˆ†**: {consistency['consistency_score']:.3f} (0-1åŒºé—´ï¼Œè¶Šé«˜è¶Šç¨³å®š)
- **å˜å¼‚ç³»æ•°**: {consistency['coefficient_of_variation']:.3f}
- **æ³¢åŠ¨æ€§è¯„ä¼°**: {consistency['volatility_assessment']}

### è¡¨ç°åˆ†å¸ƒ
- **é«˜äºå¹³å‡è¡¨ç°**: {consistency['above_average_weeks']}å‘¨ ({consistency['above_average_weeks']/basic['total_gameweeks']*100:.1f}%)
- **ä½äºå¹³å‡è¡¨ç°**: {consistency['below_average_weeks']}å‘¨ ({consistency['below_average_weeks']/basic['total_gameweeks']*100:.1f}%)

## ğŸ‘‘ é˜Ÿé•¿ç­–ç•¥åˆ†æ

### é˜Ÿé•¿ä½¿ç”¨ç»Ÿè®¡
- **ä¸åŒé˜Ÿé•¿æ•°é‡**: {captain['total_different_captains']}äºº
- **é˜Ÿé•¿å¤šæ ·æ€§æŒ‡æ•°**: {captain['captain_diversity']:.3f}
"""

        # åŠ¨æ€ç”Ÿæˆé˜Ÿé•¿åˆ†å¸ƒ
        if captain["most_used_captain"]:
            most_used = captain["most_used_captain"]
            report += f"- **æœ€å¸¸ç”¨é˜Ÿé•¿**: {most_used['name']} ({most_used['position']}, {most_used['team']}) - {most_used['games_captained']}æ¬¡ ({most_used['percentage']:.1f}%)\n"

        report += "\n### é˜Ÿé•¿ä½¿ç”¨åˆ†å¸ƒ\n"
        for captain_info in captain["captain_distribution"][:5]:  # æ˜¾ç¤ºå‰5å
            report += f"- **{captain_info['name']}** ({captain_info['position']}, {captain_info['team']}): {captain_info['games_captained']}æ¬¡ ({captain_info['percentage']:.1f}%)\n"

        report += f"""

## âš½ é˜µå‹åˆ†æ

### é˜µå‹ä½¿ç”¨ç»Ÿè®¡
- **ä½¿ç”¨é˜µå‹æ•°é‡**: {formation['total_formations_used']}ç§
- **ä¸»è¦é˜µå‹**: {formation['primary_formation']}
- **é˜µå‹å¤šæ ·æ€§**: {formation['formation_diversity']:.3f}

### é˜µå‹åˆ†å¸ƒ
"""

        # åŠ¨æ€ç”Ÿæˆé˜µå‹åˆ†å¸ƒ
        for form_info in formation["formation_distribution"]:
            report += f"- **{form_info['formation']}**: {form_info['games_used']}æ¬¡ ({form_info['percentage']:.1f}%)\n"

        report += f"""

## ğŸ“Š åŸºå‡†å¯¹æ¯”

### ä¸FPLå¹³å‡æ°´å¹³å¯¹æ¯”
- **FPLå…¸å‹å¹³å‡**: {self.config['benchmarks']['fpl_average_per_gw']:.1f}åˆ†/å‘¨
- **æœ¬ç­–ç•¥å¹³å‡**: {basic['average_points']:.1f}åˆ†/å‘¨
- **ç›¸å¯¹è¡¨ç°**: {benchmark['vs_fpl_average']['assessment']} ({benchmark['vs_fpl_average']['difference']:+.1f}åˆ†, {benchmark['vs_fpl_average']['percentage_better']:+.1f}%)

### å“è¶Šè¡¨ç°åŸºå‡†
- **å“è¶Šé—¨æ§›**: {self.config['benchmarks']['excellent_avg_threshold']:.0f}åˆ†/å‘¨
- **è·ç¦»å“è¶Š**: {benchmark['vs_excellence_threshold']['difference']:+.1f}åˆ†
- **è¾¾åˆ°å“è¶Š**: {'æ˜¯' if benchmark['vs_excellence_threshold']['reached_excellence'] else 'å¦'}

## ğŸ† æå€¼åˆ†æ

### æœ€ä½³è¡¨ç°
- **æœ€ä½³å‘¨**: GW{result['raw_data']['best_gw']['gw']} - {result['raw_data']['best_gw']['score']}åˆ†

### æœ€å·®è¡¨ç°
- **æœ€å·®å‘¨**: GW{result['raw_data']['worst_gw']['gw']} - {result['raw_data']['worst_gw']['score']}åˆ†

## âš ï¸ æ•°æ®å®Œæ•´æ€§è¯´æ˜

### {'æ•°æ®ç¼ºå¤±æƒ…å†µ' if basic['total_gameweeks'] < 38 else 'æ•°æ®å®Œæ•´æ€§'}
- **{'ç¼ºå¤±å‘¨æ•°' if basic['total_gameweeks'] < 38 else 'æ•°æ®çŠ¶æ€'}**: {str(38-basic['total_gameweeks']) + 'å‘¨' if basic['total_gameweeks'] < 38 else 'å®Œæ•´è¦†ç›–38å‘¨'}
- **å½±å“è¯„ä¼°**: {'ç»“æœåŸºäºéƒ¨åˆ†æ•°æ®ï¼Œå®é™…è¡¨ç°å¯èƒ½æœ‰å·®å¼‚' if basic['total_gameweeks'] < 38 else 'å®Œæ•´èµ›å­£æ•°æ®ï¼Œç»“æœå¯é '}
{('- **å»ºè®®**: è¡¥å……ç¼ºå¤±æ•°æ®åé‡æ–°åˆ†æä»¥è·å¾—æ›´å‡†ç¡®çš„è¯„ä¼°' + chr(10)) if basic['total_gameweeks'] < 38 else ''}

## ğŸ“‹ ç»¼åˆè¯„ä¼°

### æ€»ä½“è¯„çº§
**{performance['label']} ({performance['grade']}çº§)** - {performance['description']}

### å…³é”®ä¼˜åŠ¿
"""

        # åŠ¨æ€ç”Ÿæˆä¼˜åŠ¿
        advantages = []
        if benchmark["vs_fpl_average"]["assessment"] == "above":
            advantages.append(
                f"è¡¨ç°è¶…è¶ŠFPLå¹³å‡æ°´å¹³ {benchmark['vs_fpl_average']['percentage_better']:.1f}%"
            )
        if (
            consistency["consistency_score"]
            > self.config["benchmarks"]["good_consistency_threshold"]
        ):
            advantages.append(
                f"å…·å¤‡è‰¯å¥½çš„ç¨³å®šæ€§ (ä¸€è‡´æ€§å¾—åˆ† {consistency['consistency_score']:.3f})"
            )
        if basic["score_distribution"]["excellent_weeks"] > basic["total_gameweeks"] * 0.2:
            advantages.append(
                f"ä¼˜ç§€è¡¨ç°å‘¨æ•°å æ¯”è¾ƒé«˜ ({basic['score_distribution']['excellent_weeks']/basic['total_gameweeks']*100:.1f}%)"
            )

        for advantage in advantages:
            report += f"- {advantage}\n"

        report += "\n### æ”¹è¿›ç©ºé—´\n"

        # åŠ¨æ€ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvements = []
        if captain["captain_diversity"] < 0.2:
            improvements.append("é˜Ÿé•¿é€‰æ‹©ç›¸å¯¹å•ä¸€ï¼Œå¯è€ƒè™‘æ›´å¤šæ ·åŒ–çš„é˜Ÿé•¿ç­–ç•¥")
        if trends["trend_direction"] == "declining":
            improvements.append("èµ›å­£è¡¨ç°å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦ç­–ç•¥è°ƒæ•´ä¼˜åŒ–")
        if basic["score_distribution"]["poor_weeks"] > basic["total_gameweeks"] * 0.15:
            improvements.append("è¾ƒå·®è¡¨ç°å‘¨æ•°åå¤šï¼Œéœ€è¦æå‡é£é™©æ§åˆ¶èƒ½åŠ›")
        if not benchmark["vs_excellence_threshold"]["reached_excellence"]:
            improvements.append(
                f"è·ç¦»å“è¶Šè¡¨ç°è¿˜æœ‰ {abs(benchmark['vs_excellence_threshold']['difference']):.1f}åˆ†æå‡ç©ºé—´"
            )

        for improvement in improvements:
            report += f"- {improvement}\n"

        report += f"""

---
*æœ¬æŠ¥å‘ŠåŸºäº {basic['total_gameweeks']} å‘¨å›æµ‹æ•°æ®è‡ªåŠ¨ç”Ÿæˆï¼Œæ•°æ®é©±åŠ¨ï¼Œå®¢è§‚å‡†ç¡®*
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

        return report

    def save_analysis(self, strategy_name: str, output_path: str) -> None:
        """ä¿å­˜å®Œæ•´çš„åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        if strategy_name not in self.results:
            raise KeyError(f"ç­–ç•¥ {strategy_name} å°šæœªåŠ è½½")

        analysis = self.analyze_strategy(strategy_name)

        # è½¬æ¢ä¸ºJSONå…¼å®¹æ ¼å¼
        def convert_for_json(obj):
            if hasattr(obj, "item"):
                return obj.item()
            elif hasattr(obj, "tolist"):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            else:
                return obj

        def deep_convert(obj):
            if isinstance(obj, dict):
                return {k: deep_convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [deep_convert(v) for v in obj]
            else:
                return convert_for_json(obj)

        json_data = {
            "strategy_name": strategy_name,
            "analysis_timestamp": datetime.now().isoformat(),
            "summary": self.results[strategy_name]["summary"],
            "detailed_analysis": deep_convert(analysis),
        }

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ åˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

    def compare_strategies(self, strategy_names: list[str]) -> dict:
        """å¯¹æ¯”å¤šä¸ªç­–ç•¥"""
        if not all(name in self.results for name in strategy_names):
            missing = [name for name in strategy_names if name not in self.results]
            raise KeyError(f"ä»¥ä¸‹ç­–ç•¥å°šæœªåŠ è½½: {missing}")

        comparison = {
            "strategies": strategy_names,
            "summary_table": {},
            "rankings": {},
            "detailed_comparison": {},
        }

        # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
        for strategy in strategy_names:
            analysis = self.analyze_strategy(strategy)
            comparison["summary_table"][strategy] = {
                "total_points": analysis["basic_metrics"]["total_points"],
                "avg_points": analysis["basic_metrics"]["average_points"],
                "consistency": analysis["consistency_analysis"]["consistency_score"],
                "grade": analysis["performance_rating"]["grade"],
                "vs_fpl_avg": analysis["comparative_analysis"]["vs_fpl_average"]["difference"],
            }

        # ç”Ÿæˆæ’å
        metrics = ["total_points", "avg_points", "consistency"]
        for metric in metrics:
            sorted_strategies = sorted(
                strategy_names, key=lambda x: comparison["summary_table"][x][metric], reverse=True
            )
            comparison["rankings"][metric] = sorted_strategies

        return comparison


def create_analyzer(config_path: str = None) -> ConfigurableAnalyzer:
    """åˆ›å»ºåˆ†æå™¨å®ä¾‹"""
    return ConfigurableAnalyzer(config_path)


if __name__ == "__main__":
    try:
        print("ğŸ”§ åˆå§‹åŒ–æ•°æ®é©±åŠ¨åˆ†æå™¨...")
        analyzer = create_analyzer()

        # åŠ è½½çƒå‘˜æ˜ å°„
        analyzer.load_player_mapping()

        # åŠ è½½S0ç»“æœ
        s0_path = "/Users/carl/workspace/FP-LLM/data/backtest/results/s0_full_season_2023_24.yaml"
        if Path(s0_path).exists():
            analyzer.load_result("S0", s0_path)

            # ç”Ÿæˆæ•°æ®é©±åŠ¨æŠ¥å‘Š
            report = analyzer.generate_report(
                "S0", "/Users/carl/workspace/FP-LLM/data/backtest/analysis/s0_data_driven_report.md"
            )

            print("\nâœ… æ•°æ®é©±åŠ¨çš„S0åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        else:
            print("âš ï¸ S0ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        raise
